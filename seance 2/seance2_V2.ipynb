{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd8037af-4fdb-4dc6-bab5-c3c7fe40c4da",
   "metadata": {},
   "source": [
    "# Preparing textual data for statistics and machine learning\n",
    "\n",
    "1. Importing the dataset\n",
    "2. Cleaning the dataset\n",
    "3. Tokenization\n",
    "4. Feature extraction on a large dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b235c405",
   "metadata": {},
   "source": [
    "## Importing Data\n",
    "\n",
    "Reddit (https://www.reddit.com/) self-Posts dataset avalaible on Kaggle\n",
    "\n",
    "The data consists of 1.013M self-posts, posted from 1013 subreddits (1000 examples per class). For each post we give the subreddit, the title and content of the self-post.\n",
    "\n",
    "A subreddit is a specific online community, and the posts associated with it, on the social media website Reddit. Subreddits are dedicated to a particular topic that people write about, and they're denoted by /r/, followed by the subreddit's name, e.g., /r/gaming.\n",
    "\n",
    "For each post we give the subreddit, the title and content of the self-post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efdee6a3-b909-4dae-b994-af8e159e2444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c632ab22-6b22-4ccd-8ce4-58f784923010",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_file = \"rspct.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "954dd061-a436-4de6-ba2c-cbcb19e7805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = pd.read_csv(posts_file, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a80ce9-5c85-42e0-b4b4-51e269561517",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace1fc76-2c3e-4271-932f-1ff1fef40465",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55efb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of subredddit\n",
    "posts_df['subreddit'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b995c988",
   "metadata": {},
   "source": [
    "**subreddit_info.csv**\n",
    "\n",
    "Contains manual annotation of about 3000 subreddits :\n",
    "    \n",
    "    - a top-level category and subcategory for each subreddit, \n",
    "    \n",
    "    - a reason for exclusion if this does not appear in the data.\n",
    "\n",
    "These information can be considerered as  **metadata**: information on characteristics of the text (and not the content of the text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8efa161-55ea-4fb1-8083-56a5da03c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "subred_file = \"subreddit_info.csv\"\n",
    "subred_df=pd.read_csv(subred_file).set_index(['subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b480472-8b71-484a-a346-0950890dfb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "subred_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3126d21-3b74-43bb-816b-ebe1f4889002",
   "metadata": {},
   "outputs": [],
   "source": [
    "subred_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c329eeb7-a88f-4cca-b4a5-ac81d7594dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=posts_df.join(subred_df, on ='subreddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239bf600-5864-4b67-a052-841da3a069ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe74f78-11c3-4df9-9c5d-d90d73644b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d298d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711fbb6f-13de-4013-9d81-6b7055ae1b1e",
   "metadata": {},
   "source": [
    "### Standardizing Attributes Names\n",
    "\n",
    "Usual practise:\n",
    "- **df**: name of the dataset\n",
    "- **text**: name of the column containing text to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7259734-9e5d-4a13-8489-bea048bf1703",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb02a68",
   "metadata": {},
   "source": [
    "#### Renaming columns\n",
    "\n",
    "- selftext renamed as text\n",
    "- category_1 renamed as category\n",
    "- category_2 renamed as subcategory\n",
    "\n",
    "\n",
    " category_3, in_data and reason_for_exclusion **are suppressed (incomplete data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7339721b-0800-454c-aab2-f4235f22c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = {\n",
    "    'id':'id',\n",
    "    'subreddit':'subreddit',\n",
    "    'title':'title',\n",
    "    'selftext':'text',\n",
    "    'category_1':'category',\n",
    "    'category_2':'subcategory',\n",
    "    'category_3': None,\n",
    "    'in_data': None,\n",
    "    'reason_for_exclusion': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a941a533-a395-4e03-a071-9eb6dc84412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[c for c in column_mapping.keys() if column_mapping[c] != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43f18a66-26c3-45f9-9a77-78f1a272bd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'subreddit', 'title', 'selftext', 'category_1', 'category_2']\n"
     ]
    }
   ],
   "source": [
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8baea0af-8b87-4f01-846d-7da9cf0e8d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[columns].rename(columns=column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b677be-1db6-42e5-9d65-c24ab87b314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed08a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a217b7",
   "metadata": {},
   "source": [
    "### Selection of data for the autos category\n",
    "\n",
    "We restrict the data to the auto category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1910bbb4-128c-4f9d-8f35-dbe11ca5f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['category']=='autos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee265aa4-d6af-4ddb-879c-69fe174680d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61389955-b00a-457e-a9bd-f674f7a5b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadc135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943ca60b",
   "metadata": {},
   "source": [
    "## Python libraries\n",
    "\n",
    "Two associated Python libraries:\n",
    "\n",
    "**textacy**\n",
    "    \n",
    "        preprocessing = clean, normalize and explore raw data before processing it with spaCy*\n",
    "        \n",
    "**spaCy** : \n",
    "        \n",
    "        fundamentals = tokenization, part-of-speech tagging, dependency parsing..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0185e2d1",
   "metadata": {},
   "source": [
    "## Preliminary step: Cleaning Text Data with textacy\n",
    "\n",
    "We don't have well edited texts. There are several problems of quality that we need to take into account:\n",
    "\n",
    "- **Salutations, signatures and adresses**: usually not informative\n",
    "    \n",
    "\n",
    "- **Replies**: in case the text contains replies repeating the question, we need to eliminate the duplicated question. If not, we can introduce bias in the statistical analysis.\n",
    "    \n",
    "    \n",
    "- **Special formatting and program code**: in case, the text contain special characters, HTML entities, Mardown tags,...Necessary to eliminate these signs before the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d046998",
   "metadata": {},
   "source": [
    "- TextaCy module used to perform (preliminary/cleaning) NLP tasks on texts:\n",
    "    \n",
    "    - replacing and removing punctuation, extra whitespaces, numbers from the text before processing with spaCy\n",
    "    \n",
    "- Built upon the SpaCy module in Python\n",
    "\n",
    "https://www.geeksforgeeks.org/textacy-module-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "477ccbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.cars.com/articles/how-often-should-i-change-engine-coolant-1420680853669/<lb><lb>I have a IS 250 AWD from 2006. About 73K miles on it. I've never touched the engine radiator coolant and can't find anything on when to change this in the book. It just says 'long life 100k Toyota coolant.' <lb><lb>Does anyone get this flushed or changed at ten years?? Do I wait until 100k? \n"
     ]
    }
   ],
   "source": [
    "text=df.loc[df.index[3],'text'] # selection of text by using df.index[list]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "118b6227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "import textacy.preprocessing as tprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00d38dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = tprep.make_pipeline(\n",
    "    tprep.replace.urls,\n",
    "    tprep.remove.html_tags,\n",
    "    tprep.normalize.hyphenated_words,\n",
    "    tprep.normalize.quotation_marks,\n",
    "    tprep.normalize.unicode,\n",
    "    tprep.remove.accents,\n",
    "    tprep.remove.punctuation,\n",
    "    tprep.normalize.whitespace,\n",
    "    tprep.replace.numbers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47d63aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL have a IS _NUMBER_ AWD from _NUMBER_ About 73K miles on it I ve never touched the engine radiator coolant and can t find anything on when to change this in the book It just says long life 100k Toyota coolant Does anyone get this flushed or changed at ten years Do I wait until 100k\n"
     ]
    }
   ],
   "source": [
    "clean_text=preproc(text)\n",
    "\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c80ab3",
   "metadata": {},
   "source": [
    "### Alternative: creating a specific function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09957002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = tprep.replace.urls(text)# we replace url with text\n",
    "    text = tprep.remove.html_tags(text)\n",
    "    text = tprep.normalize.hyphenated_words(text)\n",
    "    text = tprep.normalize.quotation_marks(text)\n",
    "    text = tprep.normalize.unicode(text)\n",
    "    text = tprep.remove.accents(text)\n",
    "    text = tprep.remove.punctuation(text)\n",
    "    text = tprep.normalize.whitespace(text)\n",
    "    text = tprep.replace.numbers(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c06cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normalize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32af6ccb",
   "metadata": {},
   "source": [
    "## Linguistic Processing with spaCy\n",
    "\n",
    "- Spacy: library for linguistic data processing\n",
    "    \n",
    "- Spacy provide an integrated pipeline of processing documents:\n",
    "    \n",
    "    1. a tokenizer (by default)\n",
    "    2. a part-of-speech tagger  \n",
    "    3. a dependency parser\n",
    "    4. a named-entity recognizer\n",
    "    5. a lemmatizer\n",
    "    \n",
    "- the tokenizes is based on language-dependent rules = > fast\n",
    "\n",
    "\n",
    "- 2, 3 and 4 are based on pretrained neural models => can 10-20 times as long as tokenization\n",
    "\n",
    "- The initial input is a text\n",
    "\n",
    "- The final output is a **Doc** object\n",
    "\n",
    "- The **Doc** object contains a list of **Tokens** objects\n",
    "\n",
    "- Any range selection of tokens creates a **Span**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a3e13",
   "metadata": {},
   "source": [
    "We import spaCy one of trained pipelines for english \n",
    "\n",
    "https://spacy.io/models/en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca6c1fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ac4328b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from spacy.cli import download\n",
    "print(download('en_core_web_sm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea8206d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doc(62 tokens: \"URL have a IS _NUMBER_ AWD from _NUMBER_ About ...\")'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = textacy.make_spacy_doc(clean_text,lang=\"en_core_web_sm\")\n",
    "doc._.preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2d6db4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL have a IS _NUMBER_ AWD from _NUMBER_ About 73K miles on it I ve never touched the engine radiator coolant and can t find anything on when to change this in the book It just says long life 100k Toyota coolant Does anyone get this flushed or changed at ten years Do I wait until 100k\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84af1906",
   "metadata": {},
   "source": [
    "### Displaying tokens in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28683a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL\n",
      "have\n",
      "a\n",
      "IS\n",
      "_\n",
      "NUMBER\n",
      "_\n",
      "AWD\n",
      "from\n",
      "_\n",
      "NUMBER\n",
      "_\n",
      "About\n",
      "73\n",
      "K\n",
      "miles\n",
      "on\n",
      "it\n",
      "I\n",
      "ve\n",
      "never\n",
      "touched\n",
      "the\n",
      "engine\n",
      "radiator\n",
      "coolant\n",
      "and\n",
      "can\n",
      "t\n",
      "find\n",
      "anything\n",
      "on\n",
      "when\n",
      "to\n",
      "change\n",
      "this\n",
      "in\n",
      "the\n",
      "book\n",
      "It\n",
      "just\n",
      "says\n",
      "long\n",
      "life\n",
      "100k\n",
      "Toyota\n",
      "coolant\n",
      "Does\n",
      "anyone\n",
      "get\n",
      "this\n",
      "flushed\n",
      "or\n",
      "changed\n",
      "at\n",
      "ten\n",
      "years\n",
      "Do\n",
      "I\n",
      "wait\n",
      "until\n",
      "100k\n"
     ]
    }
   ],
   "source": [
    "for tok in doc:\n",
    "    print(tok.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5235c7",
   "metadata": {},
   "source": [
    "### Tokens have attributes \n",
    "\n",
    "    - token.is_punct  : Is the token punctuation? \n",
    "    - token.is_alpha  : Does the token consist of alphabetic characters? \n",
    "    - token.like_email : Does the token resemble an email address?\n",
    "    - token.like_url : : Does the token resemble a URL?\n",
    "    - token.is_stop : Is the token part of a “stop list”?\n",
    "    - token.lemma_ : Base form of the token, with no inflectional suffixes.\n",
    "    - token.pos : core part-of-speech categories https://universaldependencies.org/u/pos/\n",
    "            \n",
    "            \n",
    "See https://spacy.io/api/token for the list of all attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc04fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token,token.is_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fd334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token,token.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token,token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb44c013",
   "metadata": {},
   "source": [
    "### Tag-of-speech\n",
    "\n",
    "- Refers to types of words are called **part-of-speech tags**\n",
    "\n",
    "- examples: nouns, verbs, adjectives\n",
    "\n",
    "- often important to restrict the types of words used to certain categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5eceb461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL NOUN\n",
      "have VERB\n",
      "a DET\n",
      "IS PROPN\n",
      "_ PRON\n",
      "NUMBER NOUN\n",
      "_ PUNCT\n",
      "AWD PROPN\n",
      "from ADP\n",
      "_ PROPN\n",
      "NUMBER NOUN\n",
      "_ NOUN\n",
      "About ADV\n",
      "73 NUM\n",
      "K NOUN\n",
      "miles NOUN\n",
      "on ADP\n",
      "it PRON\n",
      "I PRON\n",
      "ve AUX\n",
      "never ADV\n",
      "touched VERB\n",
      "the DET\n",
      "engine NOUN\n",
      "radiator NOUN\n",
      "coolant NOUN\n",
      "and CCONJ\n",
      "can AUX\n",
      "t NOUN\n",
      "find VERB\n",
      "anything PRON\n",
      "on ADP\n",
      "when SCONJ\n",
      "to PART\n",
      "change VERB\n",
      "this PRON\n",
      "in ADP\n",
      "the DET\n",
      "book NOUN\n",
      "It PRON\n",
      "just ADV\n",
      "says VERB\n",
      "long ADJ\n",
      "life NOUN\n",
      "100k NUM\n",
      "Toyota PROPN\n",
      "coolant NOUN\n",
      "Does AUX\n",
      "anyone PRON\n",
      "get VERB\n",
      "this PRON\n",
      "flushed ADJ\n",
      "or CCONJ\n",
      "changed VERB\n",
      "at ADP\n",
      "ten NUM\n",
      "years NOUN\n",
      "Do AUX\n",
      "I PRON\n",
      "wait VERB\n",
      "until ADP\n",
      "100k NUM\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d3d1ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL NN\n",
      "have VBP\n",
      "a DT\n",
      "IS NNP\n",
      "_ DT\n",
      "NUMBER NN\n",
      "_ .\n",
      "AWD NNP\n",
      "from IN\n",
      "_ NNP\n",
      "NUMBER NN\n",
      "_ NN\n",
      "About RB\n",
      "73 CD\n",
      "K NN\n",
      "miles NNS\n",
      "on IN\n",
      "it PRP\n",
      "I PRP\n",
      "ve VBP\n",
      "never RB\n",
      "touched VBN\n",
      "the DT\n",
      "engine NN\n",
      "radiator NN\n",
      "coolant NN\n",
      "and CC\n",
      "can MD\n",
      "t NN\n",
      "find VB\n",
      "anything NN\n",
      "on IN\n",
      "when WRB\n",
      "to TO\n",
      "change VB\n",
      "this DT\n",
      "in IN\n",
      "the DT\n",
      "book NN\n",
      "It PRP\n",
      "just RB\n",
      "says VBZ\n",
      "long JJ\n",
      "life NN\n",
      "100k CD\n",
      "Toyota NNP\n",
      "coolant NN\n",
      "Does VBZ\n",
      "anyone NN\n",
      "get VB\n",
      "this DT\n",
      "flushed JJ\n",
      "or CC\n",
      "changed VBN\n",
      "at IN\n",
      "ten CD\n",
      "years NNS\n",
      "Do VBP\n",
      "I PRP\n",
      "wait VB\n",
      "until IN\n",
      "100k CD\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d20d72",
   "metadata": {},
   "source": [
    "## Lemmatization/ Stemming\n",
    "\n",
    "- Replacing words with their root: \n",
    "    - \"economic\", \"economics\", \"economically\" all replaced by the stem (the root) \"economy\"\n",
    "    - Porter stemmer (Porter 1980): standard stemming tool for English language text\n",
    "- smaller vocabulary: increase speed of execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9860c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token,token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d7537e",
   "metadata": {},
   "source": [
    "### alternative syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66acdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ae722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57e6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_alt = nlp(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e7a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_alt._.preview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e6e59c",
   "metadata": {},
   "source": [
    "### Analysis of a Doc\n",
    "\n",
    "- extracting n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c99649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy import extract\n",
    "list(extract.ngrams(doc,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44d6ce7",
   "metadata": {},
   "source": [
    "- Identifying key terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01e233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract.keyterms.textrank(doc, normalize=\"lemma\",topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c0f82",
   "metadata": {},
   "source": [
    "### Remark: We can discard some function of the spaCy pipeline\n",
    "\n",
    "We can import selected elements of the pipeline if some component are useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4021ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_2=spacy.load('en_core_web_sm', disable=[\"parser\",\"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43245c01",
   "metadata": {},
   "source": [
    "## Working with stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21685c1",
   "metadata": {},
   "source": [
    "- spaCy uses language-specific stop word lists to set the is_stop property for each token\n",
    "- Filtering stop words (and punctuation tokens) is easy\n",
    "- The list of stop words is loaded when a nlp object is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b3d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af471fbe",
   "metadata": {},
   "source": [
    "### The list of stop words can be modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67add01",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['down'].is_stop=False\n",
    "nlp.vocab['Dear'].is_stop=True\n",
    "nlp.vocab['Regards'].is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a49a57d",
   "metadata": {},
   "source": [
    "### Selection of words according to part-of-speech tags\n",
    "\n",
    "- Each token in a spaCy doc has two part-of-speech attributes:\n",
    "    - pos_\n",
    "    - tag_\n",
    "- tag_ can be language specific \n",
    "- pos_ contains the simplified tag of the universal part-of-speech tagset\n",
    "\n",
    "- A simplified form is the identification of words as nouns, verbs, adjectives, adverbs, etc.\n",
    "\n",
    "    https://spacy.io/usage/linguistic-features\n",
    "\n",
    "- pos_ can be used as an alternative to stop words\n",
    "- pronouns, prepositions, conjunctions, determiners: \n",
    "    - called **function words**\n",
    "    - their main function is to create grammatical relationships in a sentence\n",
    "    - not very informative\n",
    "\n",
    "- nouns, verbs, adjectives and adverbs: \n",
    "    - **content** words\n",
    "    - the meaning of a sentence depends on them\n",
    "    \n",
    "\n",
    "- We can use **part-of-speech tags** to select the word types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed511ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[URL, IS, NUMBER, AWD, _, NUMBER, _, K, miles, engine, radiator, coolant, t, book, life, Toyota, coolant, years]\n"
     ]
    }
   ],
   "source": [
    "nouns=[t for t in doc if t.pos_ in ['NOUN','PROPN']]\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3666b80",
   "metadata": {},
   "source": [
    "### Extracting tokens according to pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1496b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L'input doit être un objet de type doc\n",
    "tokens1=textacy.extract.words(doc, include_pos={\"ADJ\",\"NOUN\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cc3cda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL|NUMBER|NUMBER|K|miles|engine|radiator|coolant|t|book|long|life|coolant|flushed|years\n"
     ]
    }
   ],
   "source": [
    "print(*[t for t in tokens1], sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51b7f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L'input doit être un objet de type doc\n",
    "tokens2=textacy.extract.words(doc, include_pos={\"ADJ\",\"NOUN\"},min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9033814b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER|NUMBER|coolant|coolant\n"
     ]
    }
   ],
   "source": [
    "print(*[t for t in tokens2], sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2374ceb7",
   "metadata": {},
   "source": [
    "### Extracting Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7522b8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lemmas(doc,**kwargs):\n",
    "    return[t.lemma_ for t in textacy.extract.words(doc,**kwargs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3dcf1171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url|number|number|k|mile|engine|radiator|coolant|t|book|long|life|coolant|flushed|year\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_doc = extract_lemmas(doc,  include_pos={\"ADJ\",\"NOUN\"})\n",
    "print(*tokenized_doc, sep = \"|\")\n",
    "len(tokenized_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1de1950",
   "metadata": {},
   "source": [
    "### Extracting Named entities\n",
    "\n",
    "- The process of detecting entities such as people, locations, organization in texts\n",
    "- In the **Named-entity recognizer** attributes of Doc:\n",
    "    - Doc.ents\n",
    "    - Token.ent_iob_\n",
    "    - Token.ent_type_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c77a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(textacy.extract.entities(doc, include_types={\"PERSON\",\"ORG\",\"LOCATION\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49882074",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f\"({ent.text},{ent.label_})\",end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bdf304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc,style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1816c13",
   "metadata": {},
   "source": [
    "# Make a Corpus\n",
    "\n",
    "A textacy.Corpus is an ordered collection of spaCy Doc all processed by the same language pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1798330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "records=df['text']\n",
    "\n",
    "preproc_records=((preproc(text)) for text in records)\n",
    "\n",
    "corpus=textacy.Corpus(\"en_core_web_sm\",data=preproc_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "30e71825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doc(143 tokens: \"Funny story I went to college in Las Vegas This...\")'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]._.preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e3b40d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 96359, 2674826)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.n_docs, corpus.n_sents, corpus.n_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e0a4cb",
   "metadata": {},
   "source": [
    "### Transforming a corpus into an array \n",
    "\n",
    "**textacy.representations.vectorizers** : Transform a collection of tokenized docs into a doc-term matrix of shape (# docs, # unique terms), with various ways to filter or limit included terms and flexible weighting schemes for their values.\n",
    "    \n",
    "    \n",
    "https://textacy.readthedocs.io/en/latest/api_reference/representations.html#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e8b8da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = ((term.lemma_ for term in textacy.extract.words(doc,include_pos={\"ADJ\",\"NOUN\"})) for doc in corpus[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d267ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy.representations import Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4ba15709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = Vectorizer( tf_type=\"linear\")\n",
    "vectorizer = Vectorizer(tf_type=\"linear\", idf_type=\"smooth\", norm=\"l2\",min_df=3, max_df=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a54bb47",
   "metadata": {},
   "source": [
    "tf_type : specify the type of type frequency\n",
    "    tf_type = linear \n",
    "\n",
    "idf_type : Type of inverse document frequency (idf) to use for weights’ global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7a48a092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<500x883 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9483 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix = vectorizer.fit_transform(tokenized_docs)\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ada786f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10k',\n",
       " '20k',\n",
       " '22k',\n",
       " '2nd',\n",
       " '3rd',\n",
       " '4runner',\n",
       " '4th',\n",
       " '50k',\n",
       " '5th',\n",
       " 'a3',\n",
       " 'a4',\n",
       " 'able',\n",
       " 'abs',\n",
       " 'acceleration',\n",
       " 'access',\n",
       " 'accident',\n",
       " 'accord',\n",
       " 'activation',\n",
       " 'actual',\n",
       " 'ad',\n",
       " 'adapter',\n",
       " 'additional',\n",
       " 'address',\n",
       " 'adjustable',\n",
       " 'advance',\n",
       " 'advice',\n",
       " 'aero',\n",
       " 'aesthetic',\n",
       " 'aftermarket',\n",
       " 'age',\n",
       " 'air',\n",
       " 'amazing',\n",
       " 'american',\n",
       " 'amp',\n",
       " 'android',\n",
       " 'annoying',\n",
       " 'answer',\n",
       " 'app',\n",
       " 'appreciated',\n",
       " 'appropriate']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.terms_list[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a32e6f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.203154]\n",
      " [0.      ]\n",
      " [0.      ]\n",
      " [0.      ]\n",
      " [0.      ]\n",
      " [0.      ]\n",
      " [0.      ]\n",
      " [0.      ]\n",
      " [0.      ]\n",
      " [0.      ]\n",
      " [0.      ]\n",
      " [0.      ]\n",
      " [0.      ]\n",
      " [0.      ]\n",
      " [0.      ]\n",
      " [0.      ]\n",
      " [0.      ]\n",
      " [0.      ]\n",
      " [0.      ]\n",
      " [0.      ]]\n"
     ]
    }
   ],
   "source": [
    "print(doc_term_matrix[:20, vectorizer.vocabulary_terms[\"story\"]].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7f196f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tf * log((n_docs + 1) / (df + 1)) + 1'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "02cc71e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_alt = Vectorizer( tf_type=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "eac142f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<500x2790 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 11802 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix_alt = vectorizer_alt.fit_transform(tokenized_docs)\n",
    "doc_term_matrix_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4b26b34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "print(doc_term_matrix_alt[:20, vectorizer_alt.vocabulary_terms[\"story\"]].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ede0b675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tf'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_alt.weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cc3c15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
