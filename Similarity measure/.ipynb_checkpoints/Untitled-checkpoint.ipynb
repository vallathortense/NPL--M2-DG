{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4b692c",
   "metadata": {},
   "source": [
    "# Feature Engineering and Syntactic Similarity\n",
    "\n",
    "- Computation of a similiraty measure between texts\n",
    "- Impact of preprocessing on \n",
    "    - similarity measure\n",
    "    - speed of execution \n",
    "\n",
    "1 A toy dataset example\n",
    "\n",
    "2 A real dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1373b564",
   "metadata": {},
   "source": [
    "### A Toy dataset experiment\n",
    "\n",
    "We consider a text made of four sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d1934ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"It was the best of times\",\n",
    "            \"it was the worst of times\",\n",
    "            \"it was the age of wisdom\",\n",
    "            \"it was the age of foolishness\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a92a474",
   "metadata": {},
   "source": [
    "In a first step, we ignore the preprocessing step, as we have only a few sentences with not a lot of words. \n",
    "\n",
    "We want to illustrate two steps:\n",
    "    \n",
    "    1. Transforming the text into a numerical array. The dimensions of these numerical array are (number of documents × number of words) We will consider three different options:\n",
    "        1. Onehot_encoder\n",
    "        2. Word count in each document\n",
    "        3. td-idf for each document\n",
    "        \n",
    "    2. We compute a similarity indicator between each document. We will compute:\n",
    "        1. The Similarity Matrix\n",
    "        2. The Cosine similarity Matrix \n",
    "        \n",
    "\n",
    "We will use scikit-learn functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910aeea9",
   "metadata": {},
   "source": [
    "##### One-Hot Encoding with scikit-learn\n",
    " \n",
    "In that case, each text is represented by a vector (1 × number of words in the set of **all** documents) whose element are equal to 1 if the word is in the text and 0 if not. \n",
    "\n",
    "In that case, we do not take into account the number of occurrences. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
